{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main goal: Predict the number of aftershocks for a main earthquake using a Multilayer Perceptron (MLP)\n",
    "#neural network for regression.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "#personal functions to get files\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "current_directory = os.getcwd()\n",
    "clusters_folder_path = os.path.join(current_directory,'../ressources/clusters')\n",
    "clusters_filename = 'clusters_dataset.csv'\n",
    "cluster_file_path = os.path.join(clusters_folder_path,clusters_filename)\n",
    "\n",
    "\n",
    "#loading data clusters_dataset\n",
    "clusters_df = pd.read_csv(cluster_file_path)\n",
    "clusters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we add a column that will indicate if the earthquake was a mainshock, a foreshock or an aftershock\n",
    "#-1 for foreshock, 0 for mainshock; 1 for aftershock\n",
    "\n",
    "clusters_df['shockType'] = pd.Series(dtype=float)\n",
    "for index, row in clusters_df.iterrows():\n",
    "    if row['delta_days'] == 0. :\n",
    "        clusters_df.at[index,'shockType'] = 0\n",
    "    elif row['delta_days'] > 0. : \n",
    "        clusters_df.at[index,'shockType'] = 1\n",
    "    elif row['delta_days'] < 0. :\n",
    "        clusters_df.at[index,'shockType'] = -1\n",
    "\n",
    "clusters_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to be able to predict the number of aftershocks we first need to be able to have this data in the dataset.\n",
    "With the associated MC_lab(id mega cluster spatial), SC_lab(id sub cluster spatial) ST_lab(id spatio temporal cluster) we are able to identify the number of aftershocks related to the main earthquake shock in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a column for the number of aftershocks for each main earthquake\n",
    "clusters_df['aftershocks_for_main'] = clusters_df.groupby(['MC_lab', 'SC_lab', 'ST_lab'])['shockType'].transform(lambda x: (x == 1).sum())\n",
    "\n",
    "#create a column for the number of foreshocks for each main earthquake\n",
    "clusters_df['foreshocks_for_main'] = clusters_df.groupby(['MC_lab', 'SC_lab', 'ST_lab'])['shockType'].transform(lambda x: (x == -1).sum())\n",
    "\n",
    "#create a column for the total number of shocks\n",
    "clusters_df['total_shocks'] = clusters_df.groupby(['MC_lab', 'SC_lab', 'ST_lab'])['shockType'].transform('count')\n",
    "\n",
    "#display and check the new dataframe\n",
    "clusters_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create columns for minimum magnitude of aftershocks, maximum magnitude of foreshocks, and minimum magnitude of foreshocks\n",
    "clusters_df['max_aftershock_mag'] = clusters_df[clusters_df['shockType'] == 1].groupby(['MC_lab', 'SC_lab', 'ST_lab'])['mag'].transform('max')\n",
    "clusters_df['min_aftershock_mag'] = clusters_df[clusters_df['shockType'] == 1].groupby(['MC_lab', 'SC_lab', 'ST_lab'])['mag'].transform('min')\n",
    "clusters_df['max_foreshock_mag'] = clusters_df[clusters_df['shockType'] == -1].groupby(['MC_lab', 'SC_lab', 'ST_lab'])['mag'].transform('max')\n",
    "clusters_df['min_foreshock_mag'] = clusters_df[clusters_df['shockType'] == -1].groupby(['MC_lab', 'SC_lab', 'ST_lab'])['mag'].transform('min')\n",
    "\n",
    "#replace nan values with 0 (because not all mainshocks have foreshocks or aftershocks)\n",
    "clusters_df['max_aftershock_mag'] = clusters_df['max_aftershock_mag'].fillna(0)\n",
    "clusters_df['min_aftershock_mag'] = clusters_df['min_aftershock_mag'].fillna(0)\n",
    "clusters_df['max_foreshock_mag'] = clusters_df['max_foreshock_mag'].fillna(0)\n",
    "clusters_df['min_foreshock_mag'] = clusters_df['min_foreshock_mag'].fillna(0)\n",
    "\n",
    "#checking\n",
    "clusters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify rows where 'shockType' is 0 (mainshocks), this process is to have the maximum and minimum magnitudes for aftershocks and foreshocks associated with their mainshock\n",
    "mainshocks_mask = clusters_df['shockType'] == 0\n",
    "\n",
    "#find the maximum and minimum magnitudes values for aftershocks and foreshocks\n",
    "max_aftershock_mag_values = clusters_df[clusters_df['shockType'] == 1].groupby(['MC_lab', 'SC_lab', 'ST_lab'])['mag'].max()\n",
    "min_aftershock_mag_values = clusters_df[clusters_df['shockType'] == 1].groupby(['MC_lab', 'SC_lab', 'ST_lab'])['mag'].min()\n",
    "max_foreshock_mag_values = clusters_df[clusters_df['shockType'] == -1].groupby(['MC_lab', 'SC_lab', 'ST_lab'])['mag'].max()\n",
    "min_foreshock_mag_values = clusters_df[clusters_df['shockType'] == -1].groupby(['MC_lab', 'SC_lab', 'ST_lab'])['mag'].min()\n",
    "\n",
    "#change 'max_aftershock_mag', 'min_aftershock_mag', 'max_foreshock_mag' and 'min_foreshock_mag' columns on mainshock rows\n",
    "clusters_df.loc[mainshocks_mask, 'max_aftershock_mag'] = clusters_df.loc[mainshocks_mask, ['MC_lab', 'SC_lab', 'ST_lab']].apply(\n",
    "    lambda row: max_aftershock_mag_values.get(tuple(row), 0), axis=1\n",
    ")\n",
    "clusters_df.loc[mainshocks_mask, 'min_aftershock_mag'] = clusters_df.loc[mainshocks_mask, ['MC_lab', 'SC_lab', 'ST_lab']].apply(\n",
    "    lambda row: min_aftershock_mag_values.get(tuple(row), 0), axis=1\n",
    ")\n",
    "\n",
    "clusters_df.loc[mainshocks_mask, 'max_foreshock_mag'] = clusters_df.loc[mainshocks_mask, ['MC_lab', 'SC_lab', 'ST_lab']].apply(\n",
    "    lambda row: max_foreshock_mag_values.get(tuple(row), 0), axis=1\n",
    ")\n",
    "clusters_df.loc[mainshocks_mask, 'min_foreshock_mag'] = clusters_df.loc[mainshocks_mask, ['MC_lab', 'SC_lab', 'ST_lab']].apply(\n",
    "    lambda row: min_foreshock_mag_values.get(tuple(row), 0), axis=1\n",
    ")\n",
    "\n",
    "clusters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#only using mainshock data rows\n",
    "mainshocks_df = clusters_df[clusters_df['shockType'] == 0]\n",
    "\n",
    "#features and target variable\n",
    "features = mainshocks_df[['depth','mag', 'max_foreshock_mag', 'min_foreshock_mag', 'foreshocks_for_main']]\n",
    "target = mainshocks_df['aftershocks_for_main']\n",
    "\n",
    "#spliting data into training and testing sets (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "#standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#initialize MLPRegressor, 2 hidden layers(100 and 50 neurones)\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42)\n",
    "\n",
    "#train model\n",
    "mlp_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "#predictions on the test set\n",
    "predictions = mlp_regressor.predict(X_test_scaled)\n",
    "\n",
    "#evaluate model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print('Mean Squared Error on Test Set: ', mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#scatter plot of actual vs predicted values for every main earthquake(points)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "#scatter plot with fitting line of points\n",
    "sns.regplot(x=y_test, y=predictions, line_kws={'color': 'red'})\n",
    "\n",
    "#ideal diagonal line (x=y)\n",
    "plt.plot([0, max(y_test)], [0, max(y_test)], linestyle='--', color='blue', label='x=y')\n",
    "\n",
    "plt.title('Actual vs Predicted Aftershocks')\n",
    "plt.xlabel('Actual Aftershocks')\n",
    "plt.ylabel('Predicted Aftershocks')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#dataframe for actual and predicted values\n",
    "comparison_df = pd.DataFrame({'Actual': y_test, 'Predicted': predictions})\n",
    "\n",
    "#group by mainshock and sum the aftershocks\n",
    "mainshock_comparison = comparison_df.groupby(comparison_df.index).agg({'Actual': 'sum', 'Predicted': 'sum'})\n",
    "\n",
    "#plotting bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax = mainshock_comparison.plot(kind='bar', stacked=True)\n",
    "ax.set_xticks([])  # Remove x-axis ticks\n",
    "ax.set_title('Actual vs Predicted Aftershocks for Each Mainshock')\n",
    "ax.set_xlabel('Mainshock Index')\n",
    "ax.set_ylabel('Number of Aftershocks')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#still looking at only the mainshock rows\n",
    "mainshocks_df = clusters_df[clusters_df['shockType'] == 0]\n",
    "\n",
    "#selecting features and target variable\n",
    "features = mainshocks_df[['depth','mag','max_aftershock_mag', 'min_aftershock_mag', 'max_foreshock_mag', 'min_foreshock_mag']]\n",
    "target = mainshocks_df['aftershocks_for_main']\n",
    "\n",
    "#plit data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "#standardize feature\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#define the parameter grid to search\n",
    "#param_grid = {\n",
    "#    'hidden_layer_sizes': [(50,), (100,), (50, 25), (100, 50), (100, 100, 50)],\n",
    "#    'activation': ['relu', 'tanh'],\n",
    "#    'solver': ['adam', 'sgd'],\n",
    "#    'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "#    'alpha': [0.0001, 0.001, 0.01],\n",
    "#    'max_iter': [200, 300, 400],\n",
    "#    'early_stopping': [True, False],\n",
    "#}\n",
    "\n",
    "#Results(1 hour of computation):\n",
    "#Best Hyperparameters: {'activation': 'relu', 'alpha': 0.01, 'early_stopping': False, 'hidden_layer_sizes': (100, 50), 'learning_rate': 'constant', 'max_iter': 400, 'solver': 'adam'}\n",
    "#Mean Squared Error on Test Set: 1047.7739561961562\n",
    "\n",
    "#less grid parameters for 1 min computation\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(2**6, 2**7, 2**6), (2**7, 2**6)],\n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'solver': ['adam'],\n",
    "    'learning_rate': ['constant'],\n",
    "    'alpha': [0.01],\n",
    "    'max_iter': [400],\n",
    "}\n",
    "\n",
    "mlp_regressor = MLPRegressor(random_state=42)\n",
    "\n",
    "#GridSearchCV\n",
    "grid_search = GridSearchCV(mlp_regressor, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "#fit model\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "#obtain best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Hyperparameters: {best_params}')\n",
    "\n",
    "#prediction on the test set\n",
    "predictions = grid_search.predict(X_test_scaled)\n",
    "\n",
    "#evaluate model\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print('Mean Squared Error on Test Set: ', mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "F21DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
